---
title: "statistical_analysis"
author: "Dominik Dianovics"
format: html
---

# Researcher burnout and questionable research practice pilot data analysis

## Statistical analysis

### Load libraries

```{r echo = FALSE ouput = FALSE}
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("here")) install.packages("here")
if(!require("lavaan")) install.packages("lavaan")
if(!require("MVN")) install.packages("MVN")
if(!require("semPlot")) install.packages("semPlot")
if(!require("influence.SEM")) install.packages("influence.SEM")
if(!require("mice")) install.packages("mice")
if(!require("devtools")) install.packages("devtools")
if(!require("semPower")) install.packages("semPower")
if(!require("car")) install.packages("car")
if(!require("MASS")) install.packages("MASS")
library(tidyverse)
library(here)
library(ggplot2)
library(lavaan)
library(MVN)
library(semPlot)
library(influence.SEM)
library(mice)
library(devtools)
library(semPower)
library(car)
library(MASS) 
data_path <- here("data")

```

### Load data

```{r echo = FALSE ouput = FALSE}
data = read_csv(here::here("data/processed/aggregated_data.csv"))
```

### Clean data for graphs and analysis

```{r}
data = data |> 
  rename(
    WLD = workload,
    PPA = pub_pressure_attitude,
    PPR = pub_pressure_resource,
    PPS = pub_pressure_stress,
    AMB = role_ambiguity,
    CON = role_conflict,
    WLB = work_life_balance,
    OPP = opportunities,
    MNG = meaning,
    INF = influence,
    SOC = social_support,
    PAY = pay_satisfaction,
    TOL = tool_satisfaction,
    GNL = general_satisfaction,
    SEC = job_security
  )
```

# Latent variables as imagined by the Job Demands-Resources Model of Burnout

# Job demands and resources

```{r}
# Additional information:
# Scores are aggregate means of the respective scales, calculated if at least 70% of the items were answered.
# For a larger dataset, parceling or item-level analysis could be considered.
# Missing data imputation was not performed for SEM
# There is multivariate non-normality, so robust ML estimation is used.
# Data is linear, and no multicollinearity issues were detected.
# Outliers were detected using Cook's distance, and a model without outliers was also evaluated.
# The proposed model is familiar to you, however, in the final model, I have taken the pay, tool, and general satisfaction items out of the job resources latent variable due to their low loadings and high residuals.
```


## Job demands as latent variable

```{r}
#Job demands are: workload, pubplication pressure_stress, publication_pressure_attitude, role stressors such as ambiguity and conflict, work-life balance

job_demands = data |>
  dplyr::select(anonym_id, WLD, PPA, PPS, AMB, CON, WLB)

job_demands_latent = cfa(
  '
  # Measurement model
  job_demands =~ WLD + PPA + PPS + AMB + CON + WLB
  
  # Residual correlations based on theory and expected overlap
    AMB ~~ CON
  ',
  data = job_demands,
  estimator = "MLM",
  std.lv = TRUE
)
summary(job_demands_latent, fit.measures = TRUE, standardized = TRUE, modindices = TRUE)


summary_text <- capture.output(summary(job_demands_latent, fit.measures = TRUE, standardized = TRUE, modindices = TRUE))
writeLines(summary_text, here::here("analysis/tables/job_demands_summary.txt"))
pdf(here::here("analysis/figures/demands_cfa_plot.pdf"), width = 10, height = 8)
semPaths(job_demands_latent, what = "std", style = "lisrel", layout = "tree", residuals = TRUE, rotation = 4, sizeMan = 12, sizeMan2 = 6, nCharNodes = 6)
dev.off()
```

## Job resources as latent variable

```{r}
#Job resources are: opportunities, meaning, influence, social support, pay, tool, and general satisfaction, job insecurity

job_resources = data |>
  dplyr::select(anonym_id, OPP, MNG, INF, SOC, PAY, TOL, GNL, PPR, SEC)

job_resources_latent = cfa(
  '
  job_resources =~ OPP + MNG + INF + SOC + PAY + TOL + PPR + GNL + SEC
  
  # Residual correlations based on theory and expected overlap
    MNG ~~ GNL
    PAY ~~ TOL
  ',
  data = job_resources,
  estimator = "MLM",
  std.lv = TRUE
)
summary(job_resources_latent, fit.measures = TRUE, standardized = TRUE, modindices = TRUE)
modindices(job_resources_latent, sort = TRUE)

summary_text <- capture.output(summary(job_resources_latent, fit.measures = TRUE, standardized = TRUE, modindices = TRUE))
writeLines(summary_text, here::here("analysis/tables/job_resources_summary.txt"))

pdf(here::here("analysis/figures/resources_cfa_plot.pdf"), width = 10, height = 8)
semPaths(job_resources_latent, what = "stand", layout = "tree", rotation = 4, sizeMan = 12, sizeMan2 = 6, nCharNodes = 6)
dev.off()
```

## Job demands and resources plus burnout: Structural equation modeling

```{r}
jdr_b_model = '
  # Measurement model
  job_demands =~ WLD + PPA + PPS + AMB + CON + WLB
  job_resources =~ OPP + MNG + INF + SOC + PAY + TOL + PPR + GNL + SEC
  
  # Residual correlations
  AMB ~~ CON
  PAY ~~ TOL
  
  # Structural model
  burnout ~ job_demands + job_resources
'
```

### Non-imputed data approach

```{r}
jdr_b_fit = sem(jdr_b_model, data = data, std.lv = TRUE, estimator = "MLR", se = "robust.sem")

summary(jdr_b_fit, fit.measures = TRUE, standardized = TRUE, modindices = TRUE, rsquare = TRUE)
modindices(jdr_b_fit, sort = TRUE)
compRelSEM(jdr_b_fit, higher = c("job_demands", "job_resources"))  # if model uses latent variables


summary_text <- capture.output(summary(jdr_b_fit, fit.measures = TRUE, standardized = TRUE, modindices = TRUE))
writeLines(summary_text, here::here("analysis/tables/jdr_burnout_fit_summary.txt"))
```

### Visualizing model

```{r}
pdf(here::here("analysis/figures/sem_plot.pdf"), width = 10, height = 8)
semPaths(jdr_b_fit, 
         what = "stand", 
         layout = "tree2", 
         rotation = 2,
         style = "mx",
         posCol = "black",
         negCol = "black",
         nCharNodes = 0,
         residuals = TRUE,
         weighted = TRUE,
         optimizeLatRes = TRUE,
         fade = FALSE,
         edge.color = "black",
         mar = c(3, 5, 3, 5)
         )
dev.off()
```
### Evaluating model

```{r}
inspect(jdr_b_fit, "std")
semTools::compRelSEM(jdr_b_fit)  # if model uses latent variables



```
# Checking convergent validity
```{r}
jdr_measurement_model <- '
  # First-order factors
  WLD =~ copsoq_workload_1 + copsoq_workload_2 + copsoq_workload_3 + copsoq_workload_4
  PPA =~ ppqr_attitude_7 + ppqr_attitude_8 + ppqr_attitude_9 + ppqr_attitude_10 + ppqr_attitude_11 + ppqr_attitude_12
  PPS =~ ppqr_stress_1 + ppqr_stress_2 + ppqr_stress_3 + ppqr_stress_4 + ppqr_stress_5 + ppqr_stress_6
  AMB =~ role_ambiguity_01 + role_ambiguity_02 + role_ambiguity_03 + role_ambiguity_04 + role_ambiguity_05 + role_ambiguity_06
  CON =~ role_conflict_07 + role_conflict_08 + role_conflict_09 + role_conflict_10 + role_conflict_11 + role_conflict_12
  WLB =~ wlbm_1 + wlbm_2 + wlbm_3 + wlbm_4

  OPP =~ copsoq_oppor_1 + copsoq_oppor_2 + copsoq_oppor_3
  MNG =~ copsoq_meaning_1 + copsoq_meaning_2
  INF =~ copsoq_infl_1 + copsoq_infl_2 + copsoq_infl_3 + copsoq_infl_4
  SOC =~ copsoq_soc_sup_1 + copsoq_soc_sup_2 + copsoq_soc_sup_3 + copsoq_soc_sup_4
  SEC =~ jis_1 + jis_2 + jis_3 + jis_4
  PPR =~ ppqr_resources_13 + ppqr_resources_14 + ppqr_resources_15 + ppqr_resources_16 + ppqr_resources_17 + ppqr_resources_18

  # Second-order factors
  job_demands =~ WLD + PPA + PPS + AMB + CON + WLB
  job_resources =~ OPP + MNG + INF + SOC + SEC + PPR
'

jdr_fit <- cfa(
  jdr_measurement_model,
  data = processed_filtered,
  estimator = "MLR",
  std.lv = TRUE
)


stdsol <- standardizedSolution(jdr_fit)

# Extract second-order loadings for job demands
load_dem <- subset(stdsol, lhs == "job_demands" & op == "=~")[, "est.std"]

# Extract residual variances of first-order factors
resid_dem <- subset(stdsol, lhs == "job_demands" & op == "~~" & rhs != lhs)[, "est.std"]

r2_vals <- inspect(jdr_fit, "r2")

resid_dem <- 1 - r2_vals[c("WLD", "PPA", "PPS", "AMB", "CON", "WLB")]

AVE_dem <- sum(load_dem^2) / (sum(load_dem^2) + sum(resid_dem))
AVE_dem

load_res <- subset(stdsol, lhs == "job_resources" & op == "=~")[, "est.std"]
resid_res <- 1 - r2_vals[c("OPP", "MNG", "INF", "SOC", "SEC", "PPR")]

AVE_res <- sum(load_res^2) / (sum(load_res^2) + sum(resid_res))
AVE_res

htmt(jdr_fit)

```


# Assumptions for the models

### Normality

```{r}
data_normality = data |> 
  dplyr::select(burnout:WLB, -open_practice, -qrp_attitude) |>
  mutate(across(everything(), ~ as.numeric(.x)))

mvn(data_normality, mvn_test = "mardia", univariate_test = "SW")
  

# Use robust ML estimation
# estimator = "MLM" or "MLMV" or "MLR"
# se="robust.sem"
```

### Linearity assumption check

```{r}
linearity_data <- data_normality |>
  dplyr::mutate(
    job_demands_total = rowMeans(dplyr::select(cur_data(), WLD, PPA, PPS, AMB, CON, WLB), na.rm = TRUE),
    job_resources_total = rowMeans(dplyr::select(cur_data(), OPP, MNG, INF, SOC, PAY, TOL, PPR, GNL, SEC), na.rm = TRUE),
    burnout_total = burnout
  )


ggplot(linearity_data, aes(x = job_demands_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Demands → Burnout")

ggplot(linearity_data, aes(x = job_resources_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Resources → Burnout")

job_demand_items <- c("WLD", "PPA", "PPS", "AMB", "CON", "WLB")
for(item in job_demand_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_demands_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Demands"))
  )
}

job_resource_items <- c("OPP", "MNG", "INF", "SOC", "PAY", "TOL", "PPR", "GNL", "SEC")
for(item in job_resource_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_resources_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Resources"))
  )
}

lm1 <- lm(burnout_total ~ job_demands_total + job_resources_total, data = linearity_data)
plot(lm1, which = 1)
plot(lm1, which = 2)

lm_quad <- lm(burnout_total ~ job_demands_total + I(job_demands_total^2) + job_resources_total + I(job_resources_total^2),
              data = linearity_data)
anova(lm1, lm_quad)

# Linearity assumption appears to be met
```

### Multicollinearity

```{r}
correlation_matrix <- data_normality  |> 
  cor(use = "pairwise.complete.obs") |>
  round(2)

correlation_matrix_filtered <- correlation_matrix
correlation_matrix_filtered[abs(correlation_matrix_filtered) < 0.50] <- ""

correlation_matrix_filtered <- as.data.frame(correlation_matrix_filtered)
correlation_matrix_filtered


lm_vif <- lm(burnout ~ WLD + PPA + PPS + AMB + CON + WLB + OPP + MNG + INF + SOC + PAY + TOL + PPR + GNL + SEC, data = linearity_data)
vif(lm_vif)

lm_vif_structural <- lm(burnout_total ~ job_demands_total + job_resources_total, data = linearity_data)
vif(lm_vif)

# No multicollinearity issues detected (all correlations < 0.70)
```

### Outlier detection

```{r}
cooks_d <- genCookDist(jdr_b_fit, data = data)

outlier_df <- data %>%
  mutate(
    id = seq_len(nrow(.)),
    cooks_d = cooks_d,
    is_outlier_cooks = cooks_d > 1   # or 0.5 depending on preference
  )

plot(outlier_df$cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")

data_no_outliers <- outlier_df %>% 
  filter(!is_outlier_cooks) %>% 
  dplyr::select(-c(cooks_d, is_outlier_cooks))  # remove diagnostics columns


# Refit the model without outliers

fit_no_outlier <- sem(model = jdr_b_model,
                data = data_no_outliers,
                std.lv = TRUE,
                estimator = "MLR",
                missing = "listwise",
                verbose = TRUE)

summary(fit_no_outlier, fit.measures = TRUE, standardized = TRUE, modindices = TRUE, rsquare = TRUE)
modindices(fit_no_outlier, sort = TRUE)
```

# Final model

```{r}
write.csv(data_no_outliers, here::here("data/processed/data_no_outliers.csv"), row.names = FALSE)

jdr_final_model = '
  # Measurement model
  job_demands =~ WLD + PPA + PPS + AMB + CON + WLB
  job_resources =~ OPP + MNG + INF + SOC + PPR + SEC
  
  # Residual correlations
  AMB ~~ CON
  
  # Structural model
  burnout ~ job_demands + job_resources
  '

jdr_final_fit = sem(jdr_final_model, data = data_no_outliers, std.lv = TRUE, estimator = "MLR")

summary(jdr_final_fit, fit.measures = TRUE, standardized = TRUE, modindices = TRUE, rsquare = TRUE)
modindices(jdr_final_fit, sort = TRUE)

# Calculating power of final model
library(lavaan)
library(dplyr)
library(purrr)

Sigma <- lavInspect(jdr_final_fit, "cov.ov")
mu <- lavInspect(jdr_final_fit, "mean.ov")
vars <- names(mu)

posthoc_power <- function(nobs = nrow(data_no_outliers), n_sim = 1000, alpha = 0.05) {
  replicate(n_sim, {
    sim_data <- as.data.frame(MASS::mvrnorm(n = nobs, mu = mu, Sigma = Sigma))
    colnames(sim_data) <- vars
    
    sim_fit <- sem(jdr_final_model, data = sim_data, std.lv = TRUE, estimator = "MLR", missing = "fiml")
    
    est <- parameterEstimates(sim_fit)
    paths <- est %>%
      filter(op == "~", lhs == "burnout", rhs %in% c("job_demands", "job_resources")) %>%
      pull(pvalue)
    
    paths < alpha  
  }, simplify = "matrix")
}

n_pilot <- nrow(data_no_outliers)
sim_results <- posthoc_power(nobs = n_pilot, n_sim = 1000)

power_job_demands <- mean(sim_results[1, ])
power_job_resources <- mean(sim_results[2, ])

cat("Post-hoc power (job_demands → burnout):", round(power_job_demands, 3), "\n")
cat("Post-hoc power (job_resources → burnout):", round(power_job_resources, 3), "\n")

# Estimating sample size for 0.80 power in final model

Sigma <- lavInspect(jdr_final_fit, "cov.ov")   
mu <- lavInspect(jdr_final_fit, "mean.ov")   
vars <- names(mu)

simulate_power <- function(nobs, alpha = 0.05, n_sim = 500) {
  replicate(n_sim, {
    sim_data <- as.data.frame(MASS::mvrnorm(n = nobs, mu = mu, Sigma = Sigma))
    colnames(sim_data) <- vars
    
    sim_fit <- sem(jdr_final_model, data = sim_data, std.lv = TRUE, estimator = "MLR", missing = "fiml")
    
    est <- parameterEstimates(sim_fit)
    paths <- est %>%
      filter(op == "~", lhs == "burnout", rhs %in% c("job_demands", "job_resources")) %>%
      pull(pvalue)
    
    paths < alpha 
  }, simplify = "matrix")
}

sample_sizes <- seq(200, 1000, by = 200)

power_results <- tibble(N = integer(), power_job_demands = numeric(), power_job_resources = numeric())

for (n in sample_sizes) {
  cat("Simulating for N =", n, "\n")
  
  sim <- simulate_power(nobs = n, n_sim = 500)
  
  power_results <- power_results %>%
    add_row(
      N = n,
      power_job_demands = mean(sim[1, ]),
      power_job_resources = mean(sim[2, ])
    )
}

power_results %>%
  filter(power_job_demands >= 0.8 | power_job_resources >= 0.8) %>%
  arrange(N)

```

# Estimating power for the main SEM model

```{r}
semPower.getDf(jdr_b_model)

# Power to detect misspecified model

power_model_fit <- semPower(
  type = "a-priori",
  effect = 0.05,
  effect.measure = "RMSEA",
  alpha = 0.05,
  df = 100,
  power = 0.80
)

summary(power_model_fit)

# Power to detect target effect

inspect(jdr_b_fit, "std.all")


# pwrSEM was used with the following parameters:
## MODEL
### X =~ x1 + x2 + x3 + x4 + x5 + x6
### Y =~ y1 + y2 + y3 + y4 + y5 + y6 
### Z ~ Y + X

## PARAMETERS
### Factor loadings are inputed from pilot
### Regressions are inputed from pilot
### Variances are inputed from pilot
## Covariance is inputed from pilot

## Result of analysis based on 1100 simulations
### Effects of interest: burnout ~ job demands, burnout ~ job resources
### Minimum sample size for 0.80 power is 450
### Lowest power estimation was for burnout ~ job resources (0.80)

```

# Testing common method variance
```{r}
library(psych)
all_items = data[, c("burnout",
              "WLD", "PPA", "PPS", "AMB", "CON", "WLB",
              "OPP", "MNG", "INF", "SOC", "PAY", "TOL", "PPR", "GNL", "SEC")]

harman_1f <- fa(all_items,
                nfactors = 1,
                rotate = "none",
                fm = "pa")   # principal axis factoring (recommended)

print(harman_1f)
harman_1f$Vaccounted


```
