---
title: "assumption_testing"
author: "Dominik Dianovics"
format: html
---

# Researcher burnout and questionable research practice pilot data diagnostic checks

## Testing reliability, validity, and dimensionality of the used scales

### Loading packages

```{r}
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("here")) install.packages("here")
if(!require("psych")) install.packages("psych")
if(!require("semTools")) install.packages("semTools")
if(!require("semPower")) install.packages("semPower")
if(!require("MVN")) install.packages("MVN")
if(!require("car")) install.packages("car")
if(!require("lavaan")) install.packages("lavaan")
if(!require("influence.SEM")) install.packages("influence.SEM")
if(!require("simsem")) install.packages("simsem")
if(!require("tidyverse")) install.packages("tidyverse")

library(tidyverse)
library(here)
library(psych)
library(semTools)
library(semPower)
library(MVN)
library(car)
library(lavaan)
library(influence.SEM)
library(simsem)

data_path <- here("data")
```

### Loading data

```{r}
processed = read_csv(here::here("data/processed/aggregated_data.csv"))
```

### Defining items and scales

```{r}

items = c("brt_", "wld_", "scs_", "scc_",
          "opp_", "wlb_", "sec_", "inf_", "mng_", "rla_",
          "pps_", "ppa_", "ppr_"
            )

surveys = c("brt", "wld", "scs", "scc", 
            "opp", "wlb", "sec", "inf", "mng", "rla",
            "pps", "ppa", "ppr"
            )
```

## Item-level analyses

### Descriptives

```{r}
item_descriptives <- function(data, prefixes) {
  
 selected_items <- names(data)[
  Reduce(`|`, lapply(prefixes, function(pfx) startsWith(names(data), pfx)))
]
 
  desc <- psych::describe(data[, selected_items], fast = FALSE)
  
  desc_df <- desc |>
    as.data.frame() |>
    tibble::rownames_to_column("item") |>
    dplyr::select(
      item,
      n,
      mean,
      sd,
      median,
      min,
      max,
      skew,
      kurtosis,
      se
     )

  return(desc_df)
}

descriptives_table <- item_descriptives(processed, items)
```

### Univariate and multivariate normality

```{r}
data_normality = processed |> 
  dplyr::select(starts_with(items)) |>
  mutate(across(everything(), ~ as.numeric(.x)))

univariate_normality_results = mvn(data_normality, univariate_test = "SW", descriptives = FALSE, tidy = TRUE)$univariate_normality %>% 
  rename_all(tolower) %>%
  dplyr::select(variable, statistic, p.value)

multivariate_normality_results = mvn(data_normality, mvn_test = "mardia", descriptives = FALSE, tidy = TRUE)$multivariate_normality
```

### First-order reliability

```{r}
omega_function <- function(data, item_prefix) {
  items_df <- data %>%
    dplyr::select(starts_with(item_prefix))
  
   if (ncol(items_df) < 3) {
    return(data.frame(
      item = item_prefix,
      omega_total = NA_real_
    ))
  }

  omega_out <- psych::omega(
    items_df,
    nfactors = 1,
    fm = "pa",
    plot = FALSE,
    digits = 3
  )

  data.frame(
    item = item_prefix,
    omega_total = omega_out$omega.tot
  )
}

omega_results <- dplyr::bind_rows(
  lapply(items, function(prefix) omega_function(processed, prefix)))
```

### Unidimensionality

```{r}
unidimensionality_function <- function(data, item_prefix) {
  items_df <- data |>
    dplyr::select(starts_with(item_prefix))

  if (ncol(items_df) < 2) {
    return(data.frame(
      item = item_prefix,
      proportion_variance = NA,
      min_uniqueness = NA,
      max_loading = NA,
      rmsr = NA
    ))
  }
  
  fa_out <- psych::fa(
    items_df,
    nfactors = 1,
    fm = "minres",
    rotate = "none"
  )

  loadings <- as.numeric(fa_out$loadings[, 1])
  uniq <- fa_out$uniquenesses
  
  data.frame(
    item = item_prefix,
    proportion_variance = fa_out$Vaccounted["Proportion Var", 1],
    min_uniqueness = min(uniq, na.rm = TRUE),
    max_loading = max(abs(loadings), na.rm = TRUE),
    rmsr = fa_out$rms
  )
}

unidimensionality_results <- dplyr::bind_rows(
  lapply(items, function(s) unidimensionality_function(processed, s))
)

## Identifying scales that may not be unidimensional based on lower than 40% variance explained

unidimensionality_diagnostics <- unidimensionality_results %>%
  filter(proportion_variance < 0.40,
         item != "qrp_") %>%
  dplyr::pull(item)

# Checking factor number with parallel analysis

pa_table <- lapply(unidimensionality_diagnostics, function(prefix) {
  df <- processed %>% dplyr::select(starts_with(prefix))
  pa <- psych::fa.parallel(df, fa="fa", fm="minres", plot=FALSE)
  
  data.frame(
    prefix = prefix,
    eigen1 = pa$fa.values[1],
    eigen2 = pa$fa.values[2],
    random1 = pa$fa.sim[1],
    random2 = pa$fa.sim[2],
    suggested = pa$nfact
  )
}) %>% dplyr::bind_rows()
```

### Convergent validity

```{r}
fit_single_factor_cfa <- function(data, survey_prefix) {
  
  items <- names(data)[startsWith(names(data), survey_prefix)]
  
  if (length(items) < 3) {
    warning(paste("Survey", survey_prefix, "has fewer than 3 items"))
    return(NULL)
  }
  
  model <- paste0(
    "factor =~ ",
    paste(items, collapse = " + ")
  )
  
  fit <- cfa(
    model,
    data = data,
    estimator = "MLR",
    std.lv = TRUE,
    missing = "fiml"
  )
  
  list(
    survey = survey_prefix,
    fit = fit
  )
}

cfa_results <- map(
  items,
  ~ fit_single_factor_cfa(processed, .x)
)

cfa_results <- compact(cfa_results)

loadings_table <- map_df(
  cfa_results,
  function(x) {
    standardizedSolution(x$fit) |>
      filter(op == "=~") |>
      mutate(survey = x$survey) |>
      dplyr::select(survey, indicator = rhs, loading = est.std)
  }
)

compute_ave <- function(fit) {
  
  pe <- parameterEstimates(fit)
  
  loadings <- pe |>
    filter(op == "=~")
  
  errors <- pe |>
    filter(op == "~~", lhs == rhs, lhs %in% loadings$rhs)
  
  lambda_sq <- loadings$est^2
  theta     <- errors$est
  
  sum(lambda_sq) / (sum(lambda_sq) + sum(theta))
}


ave_table <- map_df(
  cfa_results,
  function(x) {
    tibble(
      survey = x$survey,
      AVE = compute_ave(x$fit)
    ) %>% 
      mutate(survey = gsub("_", "", survey))
  }
)
```

### Divergent validity

```{r}
resources_first_order = '
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
  ppr =~ ppr_13 + ppr_14 + ppr_15 + ppr_16 + ppr_17 + ppr_18
'

demands_first_order = '
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  pps =~ pps_1 + pps_2 + pps_3 + pps_4 + pps_5 + pps_6
  ppa =~ ppa_7 + ppa_8 + ppa_9 + ppa_10 + ppa_11 + ppa_12
'


run_discriminant_validity <- function(model_syntax, cutoff = 0.7) {
  fit <- cfa(
    model_syntax,
    data = processed,
    check.lv.names = FALSE
  )
  
  disc <- discriminantValidity(fit, cutoff = cutoff, level = 0.9)
  
  disc %>%
    dplyr::select(lhs, rhs, est, ci.upper, "Pr(>Chisq)") %>%
    dplyr::rename(p_value = "Pr(>Chisq)") %>%
    dplyr::mutate(p_value = round(p_value, 3))
}

discriminant_validity_resources <- run_discriminant_validity(resources_first_order)
discriminant_validity_demands <- run_discriminant_validity(demands_first_order)

survey_to_item <- setNames(items, surveys)


build_multifactor_cfa <- function(data, surveys, items) {
  
  # map survey names to item prefixes
  survey_to_item <- setNames(items, surveys)
  
  # build model for each survey
  model_parts <- map(surveys, function(sv) {
    prefix <- survey_to_item[[sv]]
    
    # select item-level variables
    indicators <- names(data)[startsWith(names(data), prefix)]
    
    # skip scales with <3 items
    # if (length(indicators) < 3) return(NULL)
    
    paste0(sv, " =~ ", paste(indicators, collapse = " + "))
  })
  
  paste(compact(model_parts), collapse = "\n")
}


multi_cfa_model <- build_multifactor_cfa(processed, surveys, items)

multi_cfa_fit <- cfa(
  multi_cfa_model,
  data = processed,
  estimator = "MLR",
  std.lv = TRUE,
  check.lv.names = FALSE
)


latent_correlations <- lavaan::inspect(multi_cfa_fit, "cor.lv")
latent_correlations

# average variance extracted greater than shared variance

# Creating AVE pairs

ave_pairs <- expand.grid(
  survey1 = surveys,
  survey2 = surveys,
  stringsAsFactors = FALSE
) %>%
  filter(survey1 < survey2) %>%
  mutate(
    ave1 = ave_table$AVE[match(survey1, ave_table$survey)],
    ave2 = ave_table$AVE[match(survey2, ave_table$survey)]
  ) %>%
  rowwise() %>%
  filter(!is.na(ave1) & !is.na(ave2))

# Extract latent correlations

ave_pairs <- ave_pairs %>%
  mutate(
    shared_variance = latent_correlations[
      cbind(survey1, survey2)
    ]^2,
    ave_greater = (ave1 > shared_variance) & (ave2 > shared_variance)
  )
```

### Multicollinearity

```{r}
correlation_matrix <- processed %>%
  dplyr::select(all_of(surveys)) %>%
  cor(use = "pairwise.complete.obs") %>%
  round(2)

correlation_matrix[abs(correlation_matrix) < 0.50] <- NA
correlation_matrix <- as.data.frame(correlation_matrix)

lm_vif <- lm(brt ~ wld + rla + wlb + pps + ppr + ppa + opp + mng + inf + scs + scc + sec, data = processed)
vif(lm_vif)

# No multicollinearity issues detected (all correlations < 0.70)
```

### Linearity

```{r}
linearity_data <- processed |>
  dplyr::mutate(
    job_demands_total = rowMeans(dplyr::select(cur_data(), wld, rla, wlb), na.rm = TRUE),
    job_resources_total = rowMeans(dplyr::select(cur_data(), opp, mng, inf, scs, scc, sec), na.rm = TRUE),
    burnout_total = brt
  )

ggplot(linearity_data, aes(x = job_demands_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Demands → Burnout")

ggplot(linearity_data, aes(x = job_resources_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Resources → Burnout")

job_demand_items <- c("wld", "rla", "wlb", "pps", "ppa")
for(item in job_demand_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_demands_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Demands"))
  )
}

job_resource_items <- c("opp", "mng", "inf", "scs", "scc", "sec", "ppr")
for(item in job_resource_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_resources_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Resources"))
  )
}

lm1 <- lm(burnout_total ~ job_demands_total + job_resources_total, data = linearity_data)
plot(lm1, which = 1)
plot(lm1, which = 2)

# Linearity assumption appears to be met
```

### Underperforming constructs

```{r}
underperforming_scales = c("rlc", "pps", "ppa")

# Detailed CFA
underperforming_cfa_results <- lapply(underperforming_scales, function(survey) {
  fit_single_factor_cfa(processed, paste0(survey, "_"))
})

# underperforming_cfa_results <- compact(underperforming_cfa_results)
lapply(underperforming_cfa_results, function(x) summary(x$fit, fit.measures = TRUE, standardized = TRUE))
lapply(underperforming_cfa_results, function(x) modificationIndices(x$fit, sort. = TRUE, maximum.number = 10))

fit_list <- lapply(underperforming_cfa_results, function(x) {
  fitMeasures(x$fit, c("cfi", "tli", "rmsea", "srmr", "aic", "bic"))
})

fit_df <- do.call(rbind, fit_list)

# Fit EFA for  RLC

rlc_fit_1 = efa(processed, ov.names = c("rlc_1", "rlc_2", "rlc_3", "rlc_4", "rlc_5", "rlc_6"), nfactors = 1, rotation = "oblimin")
rlc_fit_2 = efa(processed, ov.names = c("rlc_1", "rlc_2", "rlc_3", "rlc_4", "rlc_5", "rlc_6"), nfactors = 2, rotation = "oblimin")
summary(rlc_fit_1)
summary(rlc_fit_2)
fitMeasures(rlc_fit_1, "bic")
fitMeasures(rlc_fit_2, "bic")

# Two factor model is better for RLC based on BIC and fit indices, however, it has no theoretical grounding. No changes are made.
```

## Construct- and factor-level analyses

### Measurement model

```{r}
# Excluding aggregated constructs

jdr_measurement_items = processed %>% 
  dplyr::select(starts_with(c(items)))


jdr_measurement_model <- '
  # First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4

  
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
  
  burnout =~ brt_1 + brt_2 + brt_3 + brt_4
  
  # Job Demands
  job_demands =~ wld + rla + wlb

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  
  # burnout is regressed on job demands and job resources
  job_demands ~ burnout
  job_resources ~ burnout
  
  # Allowing theoretically related indicators and factors to correlate
  scs ~~ scc
  rla_5 ~~ rla_6
'

jdr_measurement_fit <- sem(
  jdr_measurement_model,
  data = jdr_measurement_items,
  estimator = "MLR",
  check.lv.names = FALSE,
  std.lv = TRUE
)

summary(jdr_measurement_fit, fit.measures = TRUE, standardized = TRUE)

modificationIndices(jdr_measurement_fit, sort. = TRUE, maximum.number = 10)
lavaan::inspect(jdr_measurement_fit_missing, 'coverage')
```

### Composite SEM

```{r}
jdr_composite_model <- '
  # Job Demands
  job_demands =~ wld + rla + wlb

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  
  # burnout is regressed on job demands and job resources
  brt ~ job_demands + job_resources
  
  # Allowing theoretically related indicators to correlate
  scs ~~ scc
'

jdr_composite_fit <- sem(
  jdr_composite_model,
  data = processed,
  estimator = "MLR",
  std.lv = TRUE
)

summary(jdr_composite_fit, fit.measures = TRUE, standardized = TRUE)
```


### Common method variance

```{r}
constructs = processed[, c("brt",
              "wld", "rla", "wlb",
              "opp", "mng", "inf", "scc", "scs", "sec")]

harman_1f <- fa(constructs,
                nfactors = 1,
                rotate = "none",
                fm = "pa")

harman_1f$Vaccounted
```

### Second-order reliability

```{r}
resources_factor =   '
  # First-order factors
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
'

jdr_resources_fit <- sem(
  resources_factor,
  data = processed,
  estimator = "MLR",
  check.lv.names = FALSE
)

summary(jdr_resources_fit)

semTools::compRelSEM(jdr_resources_fit, tau.eq = FALSE, higher = c("job_resources"))


demands_factor = '
  # First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  
  # Job Demands
  job_demands =~ wld + rla + wlb
'

jdr_demands_fit <- lavaan::sem(
  demands_factor,
  data = processed,
  estimator = "MLR",
  check.lv.names = FALSE
)

semTools::compRelSEM(jdr_demands_fit, tau.eq = FALSE, higher = c("job_demands"), return.df = TRUE)
```

### Second-order convergent validity

```{r}
# Fornell-Larcker criterion: AVE above .5 is ideal
AVE_second_order <- function(fit, second_order) {
  # Extract standardized solution matrices
  std <- lavaan::inspect(fit, "std.all")
  Lambda <- std$lambda   
  Beta   <- std$beta     
  
  # Identify first-order factors that load on the second-order factor
  gamma_vec <- Beta[, second_order]
  first_order <- names(gamma_vec)[!is.na(gamma_vec)]
  gamma_vec <- gamma_vec[first_order]   # remove NAs
  
  # Identify items loading on each first-order factor
  items <- rownames(Lambda)
  lambda_matrix <- Lambda[items, first_order, drop = FALSE]
  
  # λ_ij^2 (squared standardized first-order loadings)
  lambda_sq <- lambda_matrix^2
  
  # γ_j^2 (squared second-order loadings)
  gamma_sq <- gamma_vec^2
  
  # Compute item-implied loadings
  implied_loadings_sq <- sweep(lambda_sq, 2, gamma_sq, `*`)
  
  num <- sum(implied_loadings_sq)
  
  item_resid <- 1 - rowSums(lambda_sq)
  
  den <- num + sum(item_resid)
  
  AVE2 <- num / den
  
  return(AVE2)
}
AVE_second_order(jdr_resources_fit, second_order = "job_resources")
AVE_second_order(jdr_demands_fit, second_order = "job_demands")


# Credé and Harms criterion: AVE above .24 is acceptable
AVE2_raw <- function(fit, second_order) {
  # Standardized matrices
  std <- lavaan::inspect(fit, "std.all")
  Lambda <- std$lambda  
  Beta   <- std$beta    
  
  # Extract second-order loadings γ_j
  gamma_vec <- Beta[, second_order]
  first_order <- names(gamma_vec)[!is.na(gamma_vec)]
  gamma_vec <- gamma_vec[first_order]
  
  # Extract items and item loadings λ_ij
  items <- rownames(Lambda)
  lambda_matrix <- Lambda[items, first_order, drop = FALSE]
  
  # p = number of items loading on any first-order factor
  p <- sum(rowSums(lambda_matrix != 0) > 0)
  
  # Compute (λ_ij * γ_j)^2 for each item-factor combination
  implied_sq <- sweep(lambda_matrix, 2, gamma_vec, `*`)^2
  
  # Sum over all items and divide by p
  AVE2 <- sum(implied_sq) / p
  
  return(AVE2)
}

AVE2_raw(jdr_resources_fit, "job_resources")
AVE2_raw(jdr_demands_fit, "job_demands")
```

### Outlier detection

```{r}
cooks_d <- genCookDist(jdr_measurement_fit, data = processed, check.lv.names = FALSE)

outlier_df <- processed %>%
  mutate(
    id = seq_len(nrow(.)),
    cooks_d = cooks_d,
    is_outlier_cooks = cooks_d > 1   # or 0.5 depending on preference
  )

plot(outlier_df$cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")

# Sensivity analysis

data_no_outliers <- outlier_df %>% 
  filter(!is_outlier_cooks) %>% 
  dplyr::select(-c(cooks_d, is_outlier_cooks))  # remove diagnostics columns


# Refit the model without outliers

fit_no_outlier <- sem(model = jdr_measurement_fit,
                data = data_no_outliers,
                std.lv = TRUE,
                estimator = "MLR",
                verbose = TRUE)

summary(fit_no_outlier, fit.measures = TRUE, standardized = TRUE, modindices = FALSE, rsquare = FALSE)
summary(jdr_measurement_fit, fit.measures = TRUE, standardized = TRUE, modindices = FALSE, rsquare = FALSE)


anova(jdr_measurement_fit, fit_no_outlier)
```

### Power analysis for target effect using pwrSEM

```{r}
pwrsem_loadings = standardizedSolution(jdr_measurement_fit) %>%
  dplyr::filter(op == "=~") %>%
  dplyr::select(lhs, rhs, est.std)

pwrsem_reg_coefs <- standardizedSolution(jdr_measurement_fit) |>
  subset(op == "~", select = c("lhs","rhs","est.std"))

pwrsem_res_variances <- as.data.frame(lavaan::inspect(jdr_measurement_fit, "std.all")$lambda)

write.csv(pwrsem_loadings, "pwrsem_loadings.csv")
write.csv(pwrsem_reg_coefs, "pwrsem_reg_coefs.csv")
write.csv(pwrsem_res_variances, "pwrsem_res_variances.csv")
```

### Power analysis for global fit

```{r}
popModel <- model.lavaan(jdr_measurement_fit)

out <- sim(
  nRep = 100,
  model = jdr_measurement_fit,         
  generate = popModel,
  n = 400     
)
summary(out)

```

```{r}
population_model <- fitted(jdr_measurement_fit)$cov

hypothethised_model_structure_dem <- '
# First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
  
  burnout =~ brt_1 + brt_2 + brt_3 + brt_4
  
  # Job Demands
  job_demands =~ wld + rla + wlb

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  
  # burnout is regressed on job demands and job resources
  job_demands ~ %f*burnout
  job_resources ~ burnout
  
  # Allowing theoretically related indicators and factors to correlate
  scs ~~ scc
  rla_5 ~~ rla_6
'

build_H0 <- function(beta_dem, Sigma_H1) {
  
  model_H0 <- sprintf(hypothethised_model_structure_dem, beta_dem)
  
  res_H0 <- lavaan::sem(
    model_H0,
    sample.cov = Sigma_H1,
    sample.nobs = 100000,
    sample.cov.rescale = FALSE
  )
  
  list(
    Sigma_H0 = fitted(res_H0)$cov,
    df_H0    = res_H0@test[[1]]$df
  )
}


compute_power <- function(beta_dem, Sigma_H1) {
  
  h0 <- build_H0(beta_dem, Sigma_H1)
  
  out <- semPower.aPriori(
    SigmaHat = h0$Sigma_H0,
    Sigma    = Sigma_H1,
    alpha    = 0.05,
    power    = 0.80,
    df       = h0$df_H0
  )
  
  out$requiredN
}


beta_demands_grid <- seq(0, 0.5, by = 0.1)

results_dem <- data.frame(
  beta_dem = beta_demands_grid,
  requiredN = sapply(
    beta_demands_grid,
    compute_power,
    Sigma_H1 = population_model
  )
)

hypothethised_model_structure_res <- '
# First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
  
  burnout =~ brt_1 + brt_2 + brt_3 + brt_4
  
  # Job Demands
  job_demands =~ wld + rla + wlb

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  
  # burnout is regressed on job demands and job resources
  job_demands ~ burnout
  job_resources ~ %f*burnout
  
  # Allowing theoretically related indicators and factors to correlate
  scs ~~ scc
  rla_5 ~~ rla_6
'

build_H0 <- function(beta_res, Sigma_H1) {
  
  model_H0 <- sprintf(hypothethised_model_structure_res, beta_res)
  
  res_H0 <- lavaan::sem(
    model_H0,
    sample.cov = Sigma_H1,
    sample.nobs = 100000,
    sample.cov.rescale = FALSE
  )
  
  list(
    Sigma_H0 = fitted(res_H0)$cov,
    df_H0    = res_H0@test[[1]]$df
  )
}


compute_power <- function(beta_res, Sigma_H1) {
  
  h0 <- build_H0(beta_res, Sigma_H1)
  
  out <- semPower.aPriori(
    SigmaHat = h0$Sigma_H0,
    Sigma    = Sigma_H1,
    alpha    = 0.05,
    power    = 0.80,
    df       = h0$df_H0
  )
  
  out$requiredN
}


beta_resources_grid <- seq(0, -0.5, by = -0.1)

results_res <- data.frame(
  beta_res = beta_resources_grid,
  requiredN = sapply(
    beta_resources_grid,
    compute_power,
    Sigma_H1 = population_model
  )
)

results_res
results_dem

power_estimates = results_res %>% 
  full_join(results_dem) %>% 
  select(beta_res, beta_dem, requiredN)

```

# testing

```{r}
jdr_measurement_model <- '
  # First-order factors
  pps =~ pps_1 + pps_2 + pps_3 + pps_4 + pps_5 + pps_6
  ppa =~ ppa_7 + ppa_8 + ppa_9 + ppa_10 + ppa_11 + ppa_12
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4

  
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
  
  burnout =~ brt_1 + brt_2 + brt_3 + brt_4
  
  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Job Demands
  job_demands =~ wld + rla + wlb + ppa
  
  # burnout is regressed on job demands and job resources
  burnout ~  pps + job_demands + job_resources
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  job_demands ~~ pps
  
  # Allowing theoretically related indicators and factors to correlate
  scs ~~ scc
  rla_5 ~~ rla_6
  ppa_10 ~~ ppa_12
  ppa_8 ~~ ppa_9
'

jdr_measurement_fit <- sem(
  jdr_measurement_model,
  data = processed,
  estimator = "MLR",
  missing = 'fiml',
  check.lv.names = FALSE,
  std.lv = TRUE
)

summary(jdr_measurement_fit, fit.measures = TRUE, standardized = TRUE)
lavaan::inspect(jdr_measurement_fit, 'cor.lv')
modificationindices(jdr_measurement_fit, sort. = TRUE)
```

