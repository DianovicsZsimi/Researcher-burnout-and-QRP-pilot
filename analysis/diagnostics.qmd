---
title: "assumption_testing"
author: "Dominik Dianovics"
format: html
---

# Researcher burnout and questionable research practice pilot data diagnostic checks

## Testing reliability, validity, and dimensionality of the used scales

### Loading packages

```{r}
library(tidyverse)
library(here)
library(psych)
library(semTools)
library(semPower)
#install.packages("MVN")
#library(MVN)
library(car)
library(lavaan)
install.packages("influence.SEM")
library(influence.SEM)

data_path <- here("data")
```

### Loading data

```{r}
processed = read_csv(here::here("data/processed/aggregated_data.csv"))
```

### Defining items and scales

```{r}
# QRP removed

items = c("brt_", "wld_", "scs_", "scc_",
            "opp_", "wlb_", "sec_", "inf_", "mng_",
            "rla_", "rlc_")

surveys = c("brt", "wld", "scs", "scc",
            "opp", "wlb", "sec", "inf", "mng",
            "rla", "rlc")
```

## Item-level analyses

### Descriptives

```{r}
item_descriptives <- function(data, prefixes) {
  
 selected_items <- names(data)[
  Reduce(`|`, lapply(prefixes, function(pfx) startsWith(names(data), pfx)))
]
 
  desc <- psych::describe(data[, selected_items], fast = FALSE)
  
  desc_df <- desc |>
    as.data.frame() |>
    tibble::rownames_to_column("item") |>
    dplyr::select(
      item,
      n,
      mean,
      sd,
      median,
      min,
      max,
      skew,
      kurtosis,
      se
     )

  return(desc_df)
}

descriptives_table <- item_descriptives(processed, items)
```

### Univariate and multivariate normality

```{r}
data_normality = processed |> 
  dplyr::select(starts_with(items)) |>
  mutate(across(everything(), ~ as.numeric(.x)))

univariate_normality_results = mvn(data_normality, univariate_test = "SW", descriptives = FALSE, tidy = TRUE)$univariate_normality %>% 
  rename_all(tolower) %>%
  dplyr::select(variable, statistic, p.value)

multivariate_normality_results = mvn(data_normality, mvn_test = "mardia", descriptives = FALSE, tidy = TRUE)$multivariate_normality
```

### First-order reliability

```{r}
omega_function <- function(data, item_prefix) {
  items_df <- data %>%
    dplyr::select(starts_with(item_prefix))
  
   if (ncol(items_df) < 3) {
    return(data.frame(
      item = item_prefix,
      omega_total = NA_real_
    ))
  }

  omega_out <- psych::omega(
    items_df,
    nfactors = 1,
    fm = "pa",
    plot = FALSE,
    digits = 3
  )

  data.frame(
    item = item_prefix,
    omega_total = omega_out$omega.tot
  )
}

omega_results <- dplyr::bind_rows(
  lapply(items, function(prefix) omega_function(processed, prefix)))
```

### Unidimensionality

```{r}
unidimensionality_function <- function(data, item_prefix) {
  items_df <- data |>
    dplyr::select(starts_with(item_prefix))

  if (ncol(items_df) < 2) {
    return(data.frame(
      item = item_prefix,
      proportion_variance = NA,
      min_uniqueness = NA,
      max_loading = NA,
      rmsr = NA
    ))
  }
  
  fa_out <- psych::fa(
    items_df,
    nfactors = 1,
    fm = "minres",
    rotate = "none"
  )

  loadings <- as.numeric(fa_out$loadings[, 1])
  uniq <- fa_out$uniquenesses
  
  data.frame(
    item = item_prefix,
    proportion_variance = fa_out$Vaccounted["Proportion Var", 1],
    min_uniqueness = min(uniq, na.rm = TRUE),
    max_loading = max(abs(loadings), na.rm = TRUE),
    rmsr = fa_out$rms
  )
}

unidimensionality_results <- dplyr::bind_rows(
  lapply(items, function(s) unidimensionality_function(processed, s))
)

## Identifying scales that may not be unidimensional based on lower than 40% variance explained

unidimensionality_diagnostics <- unidimensionality_results %>%
  filter(proportion_variance < 0.40,
         item != "qrp_") %>%
  dplyr::pull(item)

# Checking factor number with parallel analysis

pa_table <- lapply(unidimensionality_diagnostics, function(prefix) {
  df <- processed %>% dplyr::select(starts_with(prefix))
  pa <- psych::fa.parallel(df, fa="fa", fm="minres", plot=FALSE)
  
  data.frame(
    prefix = prefix,
    eigen1 = pa$fa.values[1],
    eigen2 = pa$fa.values[2],
    random1 = pa$fa.sim[1],
    random2 = pa$fa.sim[2],
    suggested = pa$nfact
  )
}) %>% dplyr::bind_rows()
```

### Convergent validity

```{r}
fit_single_factor_cfa <- function(data, survey_prefix) {
  
  items <- names(data)[startsWith(names(data), survey_prefix)]
  
  if (length(items) < 3) {
    warning(paste("Survey", survey_prefix, "has fewer than 3 items"))
    return(NULL)
  }
  
  model <- paste0(
    "factor =~ ",
    paste(items, collapse = " + ")
  )
  
  fit <- cfa(
    model,
    data = data,
    estimator = "MLR",
    std.lv = TRUE,
    missing = "fiml"
  )
  
  list(
    survey = survey_prefix,
    fit = fit
  )
}

cfa_results <- map(
  items,
  ~ fit_single_factor_cfa(processed, .x)
)

cfa_results <- compact(cfa_results)

loadings_table <- map_df(
  cfa_results,
  function(x) {
    standardizedSolution(x$fit) |>
      filter(op == "=~") |>
      mutate(survey = x$survey) |>
      dplyr::select(survey, indicator = rhs, loading = est.std)
  }
)

compute_ave <- function(fit) {
  
  pe <- parameterEstimates(fit)
  
  loadings <- pe |>
    filter(op == "=~")
  
  errors <- pe |>
    filter(op == "~~", lhs == rhs, lhs %in% loadings$rhs)
  
  lambda_sq <- loadings$est^2
  theta     <- errors$est
  
  sum(lambda_sq) / (sum(lambda_sq) + sum(theta))
}


ave_table <- map_df(
  cfa_results,
  function(x) {
    tibble(
      survey = x$survey,
      AVE = compute_ave(x$fit)
    ) %>% 
      mutate(survey = gsub("_", "", survey))
  }
)
```

### Divergent validity

```{r}
resources_first_order = '
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
'

demands_first_order = '
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  rlc =~ rlc_1 + rlc_2 + rlc_3 + rlc_4 + rlc_5 + rlc_6
'


run_discriminant_validity <- function(model_syntax, cutoff = 0.7) {
  fit <- cfa(
    model_syntax,
    data = processed,
    check.lv.names = FALSE
  )
  
  disc <- discriminantValidity(fit, cutoff = cutoff, level = 0.9)
  
  disc %>%
    dplyr::select(lhs, rhs, est, ci.upper, "Pr(>Chisq)") %>%
    dplyr::rename(p_value = "Pr(>Chisq)") %>%
    dplyr::mutate(p_value = round(p_value, 3))
}

discriminant_validity_resources <- run_discriminant_validity(resources_first_order)
discriminant_validity_demands <- run_discriminant_validity(demands_first_order)

survey_to_item <- setNames(items, surveys)


build_multifactor_cfa <- function(data, surveys, items) {
  
  # map survey names to item prefixes
  survey_to_item <- setNames(items, surveys)
  
  # build model for each survey
  model_parts <- map(surveys, function(sv) {
    prefix <- survey_to_item[[sv]]
    
    # select item-level variables
    indicators <- names(data)[startsWith(names(data), prefix)]
    
    # skip scales with <3 items
    if (length(indicators) < 3) return(NULL)
    
    paste0(sv, " =~ ", paste(indicators, collapse = " + "))
  })
  
  paste(compact(model_parts), collapse = "\n")
}


multi_cfa_model <- build_multifactor_cfa(processed, surveys, items)

multi_cfa_fit <- cfa(
  multi_cfa_model,
  data = processed,
  estimator = "MLR",
  std.lv = TRUE,
  check.lv.names = FALSE
)


latent_correlations <- inspect(multi_cfa_fit, "cor.lv")
latent_correlations

# average variance extracted greater than shared variance

# Creating AVE pairs

ave_pairs <- expand.grid(
  survey1 = surveys,
  survey2 = surveys,
  stringsAsFactors = FALSE
) %>%
  filter(survey1 < survey2) %>%
  mutate(
    ave1 = ave_table$AVE[match(survey1, ave_table$survey)],
    ave2 = ave_table$AVE[match(survey2, ave_table$survey)]
  ) %>% 
  rowwise() %>%
  filter(!is.na(ave1) & !is.na(ave2))

# Extract latent correlations

ave_pairs <- ave_pairs %>%
  mutate(
    shared_variance = latent_correlations[
      cbind(survey1, survey2)
    ]^2,
    ave_greater = (ave1 > shared_variance) & (ave2 > shared_variance)
  )
```

### Multicollinearity

```{r}
correlation_matrix <- processed %>%
  dplyr::select(all_of(surveys)) %>%
  cor(use = "pairwise.complete.obs") %>%
  round(2)

correlation_matrix[abs(correlation_matrix) < 0.50] <- NA
correlation_matrix <- as.data.frame(correlation_matrix)

lm_vif <- lm(brt ~ wld + rla + rlc + wlb + opp + mng + inf + scs + scc + sec, data = processed)
vif(lm_vif)

# No multicollinearity issues detected (all correlations < 0.70)
```

### Linearity

```{r}
linearity_data <- processed |>
  dplyr::mutate(
    job_demands_total = rowMeans(dplyr::select(cur_data(), wld, rla, rlc, wlb), na.rm = TRUE),
    job_resources_total = rowMeans(dplyr::select(cur_data(), opp, mng, inf, scs, scc, sec), na.rm = TRUE),
    burnout_total = brt
  )

ggplot(linearity_data, aes(x = job_demands_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Demands → Burnout")

ggplot(linearity_data, aes(x = job_resources_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Resources → Burnout")

job_demand_items <- c("wld", "rla", "rlc", "wlb")
for(item in job_demand_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_demands_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Demands"))
  )
}

job_resource_items <- c("opp", "mng", "inf", "scs", "scc", "sec")
for(item in job_resource_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_resources_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Resources"))
  )
}

lm1 <- lm(burnout_total ~ job_demands_total + job_resources_total, data = linearity_data)
plot(lm1, which = 1)
plot(lm1, which = 2)

# Linearity assumption appears to be met
```

### Underperforming constructs

```{r}
underperforming_scales = c("rlc")

# Detailed CFA
underperforming_cfa_results <- lapply(underperforming_scales, function(survey) {
  fit_single_factor_cfa(processed, paste0(survey, "_"))
})

# underperforming_cfa_results <- compact(underperforming_cfa_results)
lapply(underperforming_cfa_results, function(x) summary(x$fit, fit.measures = TRUE, standardized = TRUE))
lapply(underperforming_cfa_results, function(x) modificationIndices(x$fit, sort. = TRUE, maximum.number = 10))

# Fit EFA for  RLC

rlc_fit_1 = efa(processed, ov.names = c("rlc_1", "rlc_2", "rlc_3", "rlc_4", "rlc_5", "rlc_6"), nfactors = 1, rotation = "oblimin")
rlc_fit_2 = efa(processed, ov.names = c("rlc_1", "rlc_2", "rlc_3", "rlc_4", "rlc_5", "rlc_6"), nfactors = 2, rotation = "oblimin")
summary(rlc_fit_1)
summary(rlc_fit_2)
fitMeasures(rlc_fit_1, "bic")
fitMeasures(rlc_fit_2, "bic")

# Two factor model is better for both PPR and RLC based on BIC and fit indices, however, only PPR has theoretical interpretation. Consider revising these scales in future studies. 
```

## Construct- and factor-level analyses

### Measurement model

```{r}
# Excluding aggregated constructs

jdr_measurement_items = processed %>% 
  dplyr::select(starts_with(c(items)))


jdr_measurement_model <- '
  # First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  rlc =~ rlc_1 + rlc_2 + rlc_3 + rlc_4 + rlc_5 + rlc_6
  
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4
  
  burnout =~ brt_1 + brt_2 + brt_3 + brt_4
  
  # Job Demands
  job_demands =~ wld + rla + wlb + rlc

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  
  # burnout is regressed on job demands and job resources
  burnout ~ job_demands + job_resources
  
  # Allowing theoretically related indicators and factors to correlate
  scs ~~ scc
  rla ~~ rlc
  rla_5 ~~ rla_6
  rlc_1 ~~ rlc_6
'

jdr_measurement_fit <- sem(
  jdr_measurement_model,
  data = jdr_measurement_items,
  estimator = "MLR",
  check.lv.names = FALSE,
  std.lv = TRUE
)

summary(jdr_measurement_fit, fit.measures = TRUE, standardized = TRUE)

inspect(jdr_measurement_fit, "cor.lv")

modificationIndices(jdr_measurement_fit, sort. = TRUE, maximum.number = 10)

```

### Composite SEM

```{r}
jdr_composite_model <- '
  # Job Demands
  job_demands =~ wld + rla + wlb + rlc

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
  
  # Allowing latent factors to correlate
  job_demands ~~ job_resources
  
  # burnout is regressed on job demands and job resources
  brt ~ job_demands + job_resources
  
  # Allowing theoretically related indicators to correlate
  rlc ~~ rla
  scs ~~ scc
'

jdr_composite_fit <- sem(
  jdr_composite_model,
  data = processed,
  estimator = "MLR",
  std.lv = TRUE
)

summary(jdr_composite_fit, fit.measures = TRUE, standardized = TRUE)
```


### Common method variance

```{r}
constructs = processed[, c("brt",
              "wld", "rla", "rlc", "wlb",
              "opp", "mng", "inf", "scc", "scs", "sec")]

harman_1f <- fa(constructs,
                nfactors = 1,
                rotate = "none",
                fm = "pa")

harman_1f$Vaccounted
```

### Second-order reliability

```{r}
resources_factor =   '
  # First-order factors
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4 + inf_5 + inf_6
  scs =~ scs_1 + scs_2 + scs_3 
  scc =~ scc_1 + scc_2 + scc_3
  sec =~ sec_1 + sec_2 + sec_3 + sec_4

  # Job Resources
  job_resources =~ opp + mng + inf + scs + scc + sec
'

jdr_resources_fit <- sem(
  resources_factor,
  data = processed,
  estimator = "MLR",
  check.lv.names = FALSE
)

summary(jdr_resources_fit)

semTools::compRelSEM(jdr_resources_fit, tau.eq = FALSE, higher = c("job_resources"))


demands_factor = '
  # First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  rlc =~ rlc_1 + rlc_2 + rlc_3 + rlc_4 + rlc_5 + rlc_6
  
  # Job Demands
  job_demands =~ wld + rla + wlb + rlc
'

jdr_demands_fit <- lavaan::sem(
  demands_factor,
  data = processed,
  estimator = "MLR",
  check.lv.names = FALSE
)

semTools::compRelSEM(jdr_demands_fit, tau.eq = FALSE, higher = c("job_demands"), return.df = TRUE)
```

### Second-order convergent validity

```{r}
# Fornell-Larcker criterion: AVE above .5 is ideal
AVE_second_order <- function(fit, second_order) {
  # Extract standardized solution matrices
  std <- inspect(fit, "std.all")
  Lambda <- std$lambda   # item → first-order loadings
  Beta   <- std$beta     # first-order → second-order loadings
  
  # Identify first-order factors that load on the second-order factor
  gamma_vec <- Beta[, second_order]
  first_order <- names(gamma_vec)[!is.na(gamma_vec)]
  gamma_vec <- gamma_vec[first_order]   # remove NAs
  
  # Identify items loading on each first-order factor
  # An item loads on a factor if its standardized loading is non-zero
  items <- rownames(Lambda)
  lambda_matrix <- Lambda[items, first_order, drop = FALSE]
  
  # λ_ij^2 (squared standardized first-order loadings)
  lambda_sq <- lambda_matrix^2
  
  # γ_j^2 (squared second-order loadings)
  gamma_sq <- gamma_vec^2
  
  # Compute item-implied loadings: λ_ij^2 * γ_j^2
  implied_loadings_sq <- sweep(lambda_sq, 2, gamma_sq, `*`)
  
  # Numerator: sum of squared implied loadings
  num <- sum(implied_loadings_sq)
  
  # Item residual variances: 1 - Σ λ_ij^2
  item_resid <- 1 - rowSums(lambda_sq)
  
  # Denominator: numerator + sum of item residuals
  den <- num + sum(item_resid)
  
  # AVE (second-order)
  AVE2 <- num / den
  
  return(AVE2)
}
AVE_second_order(jdr_resources_fit, second_order = "job_resources")
AVE_second_order(jdr_demands_fit, second_order = "job_demands")


# Credé and Harms criterion: AVE above .24 is acceptable
AVE2_raw <- function(fit, second_order) {
  # Standardized matrices
  std <- inspect(fit, "std.all")
  Lambda <- std$lambda   # items → first-order loadings
  Beta   <- std$beta     # first-order → second-order loadings
  
  # Extract second-order loadings γ_j
  gamma_vec <- Beta[, second_order]
  first_order <- names(gamma_vec)[!is.na(gamma_vec)]
  gamma_vec <- gamma_vec[first_order]
  
  # Extract items and item loadings λ_ij
  items <- rownames(Lambda)
  lambda_matrix <- Lambda[items, first_order, drop = FALSE]
  
  # p = number of items loading on any first-order factor
  p <- sum(rowSums(lambda_matrix != 0) > 0)
  
  # Compute (λ_ij * γ_j)^2 for each item-factor combination
  implied_sq <- sweep(lambda_matrix, 2, gamma_vec, `*`)^2
  
  # Sum over all items and divide by p
  AVE2 <- sum(implied_sq) / p
  
  return(AVE2)
}

AVE2_raw(jdr_resources_fit, "job_resources")
AVE2_raw(jdr_demands_fit, "job_demands")
```

### Outlier detection

```{r}
cooks_d <- genCookDist(jdr_measurement_fit, data = processed, check.lv.names = FALSE)

outlier_df <- processed %>%
  mutate(
    id = seq_len(nrow(.)),
    cooks_d = cooks_d,
    is_outlier_cooks = cooks_d > 1   # or 0.5 depending on preference
  )

plot(outlier_df$cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")

# Sensivity analysis

data_no_outliers <- outlier_df %>% 
  filter(!is_outlier_cooks) %>% 
  dplyr::select(-c(cooks_d, is_outlier_cooks))  # remove diagnostics columns


# Refit the model without outliers

fit_no_outlier <- sem(model = jdr_measurement_fit,
                data = data_no_outliers,
                std.lv = TRUE,
                estimator = "MLR",
                missing = "listwise",
                verbose = TRUE)

summary(fit_no_outlier, fit.measures = TRUE, standardized = TRUE, modindices = TRUE, rsquare = TRUE)

fit_no_outlier$AIC
```


### Power analysis for construct-level SEM

### Power analysis for item-level SEM