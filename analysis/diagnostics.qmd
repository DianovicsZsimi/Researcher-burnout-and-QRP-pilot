---
title: "assumption_testing"
author: "Dominik Dianovics"
format: html
---

# Researcher burnout and questionable research practice pilot data diagnostic checks

## Testing reliability, validity, and dimensionality of the used scales

### Loading packages

```{r}
library(tidyverse)
library(here)
library(psych)
library(semTools)
library(semPower)
library(MVN)
library(car)
library(lavaan)

data_path <- here("data")
```

### Loading data

```{r}
processed = read_csv(here::here("data/processed/aggregated_data.csv"))
```

### Defining items and scales

```{r}
# QRP removed

items = c("brt_", "pps_", "ppa_", "ppr_", "wld_", "soc_",
            "opp_", "wlb_", "sec_", "inf_", "mng_",
            "rla_", "rlc_", "pay_", "tol_", "gnl_")

surveys = c("brt", "pps", "ppa", "ppr", "wld", "soc",
            "opp", "wlb", "sec", "inf", "mng",
            "rla", "rlc", "pay", "tol", "gnl")
```

## Item-level analyses

### Descriptives

```{r}
item_descriptives <- function(data, prefixes) {
  
 selected_items <- names(data)[
  Reduce(`|`, lapply(prefixes, function(pfx) startsWith(names(data), pfx)))
]
 
  desc <- psych::describe(data[, selected_items], fast = FALSE)
  
  desc_df <- desc |>
    as.data.frame() |>
    tibble::rownames_to_column("item") |>
    dplyr::select(
      item,
      n,
      mean,
      sd,
      median,
      min,
      max,
      skew,
      kurtosis,
      se
     )

  return(desc_df)
}

descriptives_table <- item_descriptives(processed, items)
```

### Univariate and multivariate normality

```{r}
data_normality = processed |> 
  dplyr::select(starts_with(items)) |>
  mutate(across(everything(), ~ as.numeric(.x)))

univariate_normality_results = mvn(data_normality, univariate_test = "SW", descriptives = FALSE, tidy = TRUE)$univariate_normality %>% 
  rename_all(tolower) %>%
  dplyr::select(variable, statistic, p.value)

multivariate_normality_results = mvn(data_normality, mvn_test = "mardia", descriptives = FALSE, tidy = TRUE)$multivariate_normality
```

### First-order reliability

```{r}
omega_function <- function(data, item_prefix) {
  items_df <- data %>%
    dplyr::select(starts_with(item_prefix))
  
   # remove items with zero variance
 # items_df <- items_df[, apply(items_df, 2, function(x) var(x, na.rm = TRUE) > 0), drop = FALSE]
  
   if (ncol(items_df) < 3) {
    return(data.frame(
      item = item_prefix,
      omega_total = NA_real_
    ))
  }

  omega_out <- psych::omega(
    items_df,
    nfactors = 1,
    fm = "pa",
    plot = FALSE
  )

  data.frame(
    item = item_prefix,
    omega_total = omega_out$omega.tot
  )
}

omega_results <- dplyr::bind_rows(
  lapply(items, function(prefix) omega_function(processed, prefix)))
```

### Unidimensionality

```{r}
unidimensionality_function <- function(data, item_prefix) {
  items_df <- data |>
    dplyr::select(starts_with(item_prefix))

  if (ncol(items_df) < 2) {
    return(data.frame(
      item = item_prefix,
      proportion_variance = NA,
      min_uniqueness = NA,
      max_loading = NA,
      rmsr = NA
    ))
  }
  
  fa_out <- psych::fa(
    items_df,
    nfactors = 1,
    fm = "minres",
    rotate = "none"
  )

  loadings <- as.numeric(fa_out$loadings[, 1])
  uniq <- fa_out$uniquenesses
  
  data.frame(
    item = item_prefix,
    proportion_variance = fa_out$Vaccounted["Proportion Var", 1],
    min_uniqueness = min(uniq, na.rm = TRUE),
    max_loading = max(abs(loadings), na.rm = TRUE),
    rmsr = fa_out$rms
  )
}

unidimensionality_results <- dplyr::bind_rows(
  lapply(items, function(s) unidimensionality_function(processed, s))
)

## Identifying scales that may not be unidimensional based on lower than 40% variance explained

unidimensionality_diagnostics <- unidimensionality_results %>%
  filter(proportion_variance < 0.40,
         item != "qrp_") %>%
  dplyr::pull(item)

# Checking factor number with parallel analysis

pa_table <- lapply(unidimensionality_diagnostics, function(prefix) {
  df <- processed %>% dplyr::select(starts_with(prefix))
  pa <- psych::fa.parallel(df, fa="fa", fm="minres", plot=FALSE)
  
  data.frame(
    prefix = prefix,
    eigen1 = pa$fa.values[1],
    eigen2 = pa$fa.values[2],
    random1 = pa$fa.sim[1],
    random2 = pa$fa.sim[2],
    suggested = pa$nfact
  )
}) %>% dplyr::bind_rows()
```

### Convergent validity

```{r}
fit_single_factor_cfa <- function(data, survey_prefix) {
  
  items <- names(data)[startsWith(names(data), survey_prefix)]
  
  # Safety check
  if (length(items) < 3) {
    warning(paste("Survey", survey_prefix, "has fewer than 3 items"))
    return(NULL)
  }
  
  # Build CFA model string
  model <- paste0(
    "factor =~ ",
    paste(items, collapse = " + ")
  )
  
  fit <- cfa(
    model,
    data = data,
    estimator = "MLR",
    std.lv = TRUE,
    missing = "fiml"
  )
  
  list(
    survey = survey_prefix,
    fit = fit
  )
}

cfa_results <- map(
  items,
  ~ fit_single_factor_cfa(processed, .x)
)

cfa_results <- compact(cfa_results)

loadings_table <- map_df(
  cfa_results,
  function(x) {
    standardizedSolution(x$fit) |>
      filter(op == "=~") |>
      mutate(survey = x$survey) |>
      dplyr::select(survey, indicator = rhs, loading = est.std)
  }
)

compute_ave <- function(fit) {
  
  pe <- parameterEstimates(fit)
  
  loadings <- pe |>
    filter(op == "=~")
  
  errors <- pe |>
    filter(op == "~~", lhs == rhs, lhs %in% loadings$rhs)
  
  lambda_sq <- loadings$est^2
  theta     <- errors$est
  
  sum(lambda_sq) / (sum(lambda_sq) + sum(theta))
}


ave_table <- map_df(
  cfa_results,
  function(x) {
    tibble(
      survey = x$survey,
      AVE = compute_ave(x$fit)
    ) %>% 
      mutate(survey = gsub("_", "", survey))
  }
)


```

### Divergent validity

```{r}
survey_to_item <- setNames(items, surveys)


build_multifactor_cfa <- function(data, surveys, items) {
  
  # map survey names to item prefixes
  survey_to_item <- setNames(items, surveys)
  
  # build model for each survey
  model_parts <- map(surveys, function(sv) {
    prefix <- survey_to_item[[sv]]
    
    # select item-level variables
    indicators <- names(data)[startsWith(names(data), prefix)]
    
    # skip scales with <3 items
    if (length(indicators) < 3) return(NULL)
    
    paste0(sv, " =~ ", paste(indicators, collapse = " + "))
  })
  
  paste(compact(model_parts), collapse = "\n")
}


multi_cfa_model <- build_multifactor_cfa(processed, surveys, items)

multi_cfa_fit <- cfa(
  multi_cfa_model,
  data = processed,
  estimator = "MLR",
  std.lv = TRUE,
  check.lv.names = FALSE
)


latent_correlations <- inspect(multi_cfa_fit, "cor.lv")
latent_correlations

htmt(multi_cfa_model, processed)

# average variance extracted greater than shared variance

# Creating AVE pairs

ave_pairs <- expand.grid(
  survey1 = surveys,
  survey2 = surveys,
  stringsAsFactors = FALSE
) %>%
  filter(survey1 < survey2) %>%
  mutate(
    ave1 = ave_table$AVE[match(survey1, ave_table$survey)],
    ave2 = ave_table$AVE[match(survey2, ave_table$survey)]
  ) %>% 
  rowwise() %>%
  filter(!is.na(ave1) & !is.na(ave2))

# Extract latent correlations

ave_pairs <- ave_pairs %>%
  mutate(
    shared_variance = latent_correlations[
      cbind(survey1, survey2)
    ]^2,
    ave_greater = (ave1 > shared_variance) & (ave2 > shared_variance)
  )
  
```

### Multicollinearity

```{r}
correlation_matrix <- processed %>%
  dplyr::select(all_of(surveys)) %>%
  cor(use = "pairwise.complete.obs") %>%
  round(2)

correlation_matrix[abs(correlation_matrix) < 0.50] <- NA
correlation_matrix <- as.data.frame(correlation_matrix)

lm_vif <- lm(brt ~ wld + ppa + pps + rla + rlc + wlb + opp + mng + inf + soc + ppr + sec, data = processed)
vif(lm_vif)

# No multicollinearity issues detected (all correlations < 0.70)

```

### Linearity

```{r}
linearity_data <- processed |>
  dplyr::mutate(
    job_demands_total = rowMeans(dplyr::select(cur_data(), wld, ppa, pps, rla, rlc, wlb), na.rm = TRUE),
    job_resources_total = rowMeans(dplyr::select(cur_data(), opp, mng, inf, soc, pay, tol, ppr, gnl, sec), na.rm = TRUE),
    burnout_total = brt
  )

ggplot(linearity_data, aes(x = job_demands_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Demands → Burnout")

ggplot(linearity_data, aes(x = job_resources_total, y = burnout_total)) +
  geom_point() + geom_smooth(method = "loess") + ggtitle("Job Resources → Burnout")

job_demand_items <- c("wld", "ppa", "pps", "rla", "rlc", "wlb")
for(item in job_demand_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_demands_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Demands"))
  )
}

job_resource_items <- c("opp", "mng", "inf", "soc", "pay", "tol", "ppr", "gnl", "sec")
for(item in job_resource_items) {
  print(
    ggplot(linearity_data, aes_string(x = "job_resources_total", y = item)) +
      geom_point() + geom_smooth(method = "loess") +
      ggtitle(paste(item, "vs Job Resources"))
  )
}

lm1 <- lm(burnout_total ~ job_demands_total + job_resources_total, data = linearity_data)
plot(lm1, which = 1)
plot(lm1, which = 2)

# Linearity assumption appears to be met
```

### Underperforming constructs

```{r}
underperforming_scales = c("pps", "ppr", "rlc")

# Detailed CFA

underperforming_cfa_results <- lapply(underperforming_scales, function(survey) {
  fit_single_factor_cfa(processed, paste0(survey, "_"))
})

# underperforming_cfa_results <- compact(underperforming_cfa_results)
lapply(underperforming_cfa_results, function(x) summary(x$fit, fit.measures = TRUE, standardized = TRUE))
lapply(underperforming_cfa_results, function(x) modificationIndices(x$fit, sort. = TRUE, maximum.number = 10))

# Fit EFA for PPR and RLC (for PPS the parallel analysis suggested unidimensionality)

ppr_fit_1 = efa(processed, ov.names = c("ppr_13", "ppr_14", "ppr_15", "ppr_16", "ppr_17", "ppr_18"), nfactors = 1, rotation = "oblimin")
ppr_fit_2 = efa(processed, ov.names = c("ppr_13", "ppr_14", "ppr_15", "ppr_16", "ppr_17", "ppr_18"), nfactors = 2, rotation = "oblimin")
summary(ppr_fit_1)
summary(ppr_fit_2)
fitMeasures(ppr_fit_1, "bic")
fitMeasures(ppr_fit_2, "bic")

rlc_fit_1 = efa(processed, ov.names = c("rlc_1", "rlc_2", "rlc_3", "rlc_4", "rlc_5", "rlc_6"), nfactors = 1, rotation = "oblimin")
rlc_fit_2 = efa(processed, ov.names = c("rlc_1", "rlc_2", "rlc_3", "rlc_4", "rlc_5", "rlc_6"), nfactors = 2, rotation = "oblimin")
summary(rlc_fit_1)
summary(rlc_fit_2)
fitMeasures(rlc_fit_1, "bic")
fitMeasures(rlc_fit_2, "bic")

# Two factor model is better for both PPR and RLC based on BIC and fit indices, however, only PPR has theoretical interpretation. Consider revising these scales in future studies. 
```

## Construct- and factor-level analyses

## Measurement model

```{r}
jdr_measurement_model <- '
  # First-order factors
  wld =~ wld_1 + wld_2 + wld_3 + wld_4
  ppa =~ ppa_7 + ppa_8 + ppa_9 + ppa_10 + ppa_11 + ppa_12
  pps =~ pps_1 + pps_2 + pps_3 + pps_4 + pps_5 + pps_6
  rla =~ rla_1 + rla_2 + rla_3 + rla_4 + rla_5 + rla_6
  rlc =~ rlc_1 + rlc_2 + rlc_3 + rlc_4 + rlc_5 + rlc_6
  wlb =~ wlb_1 + wlb_2 + wlb_3 + wlb_4
  
  opp =~ opp_1 + opp_2 + opp_3
  mng =~ mng_1 + mng_2 
  inf =~ inf_1 + inf_2 + inf_3 + inf_4
  soc =~ soc_1 + soc_2 + soc_3 + soc_4
  ppr =~ ppr_13 + ppr_14 + ppr_15 + ppr_16 + ppr_17 + ppr_18
  sec =~ sec_1 + sec_2 + sec_3 + sec_4


  # Job Demands
  job_demands =~ wld + ppa + pps + rla + rlc + wlb

  # Job Resources
  job_resources =~ opp + mng + inf + soc + ppr + sec

  # Burnout
  burnout =~ brt
'

jdr_measurement_fit <- cfa(
  jdr_measurement_model,
  data = processed,
  estimator = "MLR",
  std.lv = TRUE
)
  
```


### Common method variance

```{r}
constructs = processed[, c("brt",
              "wld", "ppa", "pps", "rla", "rlc", "wlb",
              "opp", "mng", "inf", "soc", "ppr", "sec")]

harman_1f <- fa(constructs,
                nfactors = 1,
                rotate = "none",
                fm = "pa")

harman_1f$Vaccounted
```

### Second-order reliability

```{r}
semTools::compRelSEM(jdr_measurement_fit, higher = c("job_demands", "job_resources"))
```

### Second-order convergent validity

```{r}
stdsol <- standardizedSolution(jdr_measurement_fit)

load_dem <- subset(stdsol, lhs == "job_demands" & op == "=~")[, "est.std"]

resid_dem <- subset(stdsol, lhs == "job_demands" & op == "~~" & rhs != lhs)[, "est.std"]

r2_vals <- inspect(jdr_measurement_fit, "r2")

resid_dem <- 1 - r2_vals[c("wld", "ppa", "pps", "rla", "rlc", "wlb")]

AVE_dem <- sum(load_dem^2) / (sum(load_dem^2) + sum(resid_dem))
AVE_dem

load_res <- subset(stdsol, lhs == "job_resources" & op == "=~")[, "est.std"]
resid_res <- 1 - r2_vals[c("opp", "mng", "inf", "soc", "sec", "ppr")]

AVE_res <- sum(load_res^2) / (sum(load_res^2) + sum(resid_res))
AVE_res

lambda_items <- subset(stdsol, op == "=~" & lhs %in% c("wld", "ppa", "pps", "rla", "rlc", "wlb"))[, c("lhs","rhs","est.std")]
gamma_factors <- subset(stdsol, op == "=~" & lhs == "job_demands")[, c("lhs","rhs","est.std")]
lambda_items$gamma <- gamma_factors$est.std[ match(lambda_items$lhs, gamma_factors$rhs) ]
lambda_items$combined <- lambda_items$est.std * lambda_items$gamma
p <- nrow(lambda_items)

AVE_dem <- sum(lambda_items$combined^2) / p
AVE_dem

```
### Second-order divergent validity

### Multicollinearity

### Outlier detection

### Power analysis for construct-level SEM

### Power analysis for item-level SEM

### Parceling strategy